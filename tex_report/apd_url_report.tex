\documentclass[11pt, a4paper]{article}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}   % if you want the fonts
\usepackage{amssymb}    % if you want extra symbols
\usepackage{graphicx}  % need for figures
\usepackage{geometry}
\usepackage{tocloft}
\usepackage{secdot}
\sectiondot{section}
\sectiondot{subsection}
\renewcommand{\cftsecaftersnum}{.}
\renewcommand{\cftsubsecaftersnum}{.}		
\usepackage[utf8]{inputenc}
\usepackage{ragged2e}
\usepackage[hidelinks]{hyperref}

\begin{document}	
\begin{titlepage}
\centering
\LARGE{\textsc{Analyse et classification d'urls à des fins de segmentation automatique par groupes de ciblage}}\\
\vfill
\large Matthieu Brito Antunes\\
\large Data Scientist\\
\large Tradelab - Paris\\
\large septembre 2020\\
\end{titlepage}

\newpage
\tableofcontents
\listoftables
\listoffigures
\thispagestyle{empty}

\newpage
\pagenumbering{arabic}

\section{Introduction}
\label{sec:intro}

L'ensemble des adresses web (URL) des pages visitées sur Internet peut constituer une source pertinente lorsque l'on souhaite réfléchir à une méthode de ciblage publicitaire. Les informations extraites à partir du traitement des URL permettent de comprendre les mécanismes qui régissent le regroupement d'utilisateurs, ou \textit{segmentation}. 

La segmentation d'utilisateurs est une étape cruciale dans le processus de ciblage publicitaire en ligne et peut s'avérer très chronophage. Il apparaît donc essentiel de chercher à utiliser toute information qui permettrait de comprendre comment tendre vers son automatisation totale.

Les informations contenues dans la structure des URL nous renseignent principalement sur l'organisation globale d'un site web, et la manière dont sont créées les URL associées aux nouvelles pages. Ceci permet de construire des regroupements de pages web sans avoir à analyser leur contenu. Ces regroupements pourraient idéalement être réalisés de manière automatique, à partir d'un modèle de \textit{clustering} qui recevrait une URL en entrée et fournirait en sortie le groupe auquel elle appartient après avoir analysé sa structure.

Ce rapport présente le travail de construction d'un modèle d'analyse et de classification automatique d'URL. La première partie du rapport est centrée sur l'étude de la problématique et des enjeux associés. Diverses pistes d'analyse de structure d'URL sont proposées. La seconde partie du rapport offre une vision théorique de la solution proposée pour automatiser la création de segments à partir des données d'URL collectées chaque jour chez Jellyfish France. La dernière partie du rapport présente la mise en place de la solution et les résultats obtenus sur des données récoltées sur une période fixe.

\section{Segmentation d'URL}

Le nombre croissant de visites de pages web et de liens échangés constitue un défi auquel doivent faire face les ingénieurs et chercheurs appliqués dans le vaste domaine du \textit{web mining}. L'analyse de l'information contenue dans les pages web et les renseignements que des séquences d'URL peuvent apporter sont cruciaux lorsqu'il s'agit de mettre en place des stratégies visant à optimiser l'utilisation d'Internet, que ce soit à des fins publicitaires (ciblage, analyse de parcours de navigation), de sécurité (détection d'URL de pages malveillantes), de construction de moteurs de recherche, pour l'archivage du web, etc.

\section{Analyse de la structure des URL et premiers essais de segmentation automatique via \textit{pattern trees}}

\subsection{Structure classique d'une URL}

Sur Internet, les pages web regroupent divers formats de données (texte, image, son...) et sont repérées par une chaîne de caractères renseignant deux choses :
\begin{itemize}
	\item leur emplacement
	\item le protocole internet permettant d'y accéder\footnote{Les protocoles les plus connus sont ceux que l'on rencontre tous les jours dans la barre d'adresse de notre navigateur : \texttt{http} et \texttt{https} pour \textit{HyperText Transfer Protocol (Secure)}.}
\end{itemize}

Cette chaîne de caractères, appelée \textit{adresse web} ou encore URL (pour \textit{Uniform Resource Locator}) est générée pour chaque nouvelle page ajoutée à un site, et permet donc à l'utilisateur de savoir où il se trouve. Encore faut-il qu'il sache la lire ! On retrouve en effet plusieurs éléments constitutifs d'une URL, qui peuvent être présent ou absents selon le type de site sur lequel l'utilisateur se trouve. Globalement, une URL peut être décrite comme suit :

\begin{center}

\texttt{http://www.bing.com/images/search?q=hp\&form=bifd}

\end{center}

avec 

\begin{itemize}
	\item \texttt{http} : \textit{scheme} 
	\item \texttt{www.bing.com} : \textit{authority}
	\item \texttt{images/search} : \textit{path}
	\item \texttt{q=hp\&form=bifd} : \textit{query}
\end{itemize}

\subsection{Analyse de la structure des URLs via \textit{pattern trees}}

La première approche vise à mettre en évidence des structures caractéristiques au sein des URL en faisant appel à des outils analogues aux arbres de décision~\cite{lei}. L'idée est ici d'apprendre à transformer des URL doublons en une forme canonique à l'aide de règles de réécriture. A partir des règles d'écriture dégagées après analyse des URL disponibles, la segmentation automatique est possible sur de nouvelles URL, en considérant que les URL soumises aux mêmes règles de réécriture appartiennent au même segment.

L'ensemble des règles de réécriture est regroupé dans un \textit{pattern tree}, i.e. un groupe de structures d'URL organisé selon une certaine hiérarchie. Au sein de cet arbre, chaque nœud représente un groupe d'URL ayant la même structure syntaxique. Cet arbre permet de rassembler l'ensemble de l'information statistique relative aux URL à disposition, augmentant ainsi la robustesse et la fiabilité du processus de segmentation.
 C'est sur cette idée que la première approche de ce travail est basée pour essayer de créer automatiquement des segments d'URL présentant des similitudes sans avoir à analyser le contenu des pages auxquelles elles renvoient (fig.~\ref{pattern_tree}).

\begin{figure}[!h]
	\center
	\includegraphics[scale=0.6]{pattern_tree.png}
	\caption{Exemple de \textit{pattern tree} construit à partir du site \texttt{www.wretch.cc}}
	\label{pattern_tree}
\end{figure}

\subsection{Résultats de transformations d'URL et orientation vers une autre méthode}

L'utilisation d'un \textit{pattern tree} pour regrouper des URL en se basant sur leurs similarités syntaxiques présente l'inconvénient d'être principalement adaptée aux URL d'un seul et même site web. Lorsque l'algorithme de construction du \textit{pattern tree} est fourni en URL trop différentes les unes des autres, des résultats peu significatifs peuvent être observés, le principal (et le plus fâcheux) étant l'obtention d'un unique segment d'URL transformées en la forme \texttt{http://*/*/*}. On s'aperçoit qu'au sein de ce segment toutes les URL sont regroupées et qu'il ne présente par conséquent aucune spécificité exploitable dans l'optique d'un ciblage publicitaire d'utilisateurs.

\section{Approche statistique de la classification automatique d'URL}

La seconde approche adoptée s'est basée sur des outils largement répandus dans le domaine des systèmes de recommandation et d'analyse des préférences d'utilisateurs~\cite{morichetta}. Le principe de cette approche statistique est basé sur le calcul de distance entre les objets à classer. D'une façon générale, retenons que tout objet que l'on cherche à faire passer par un processus de classification automatique (ou segmentation) peut être représenté par un vecteur. En d'autres termes, il est possible de projeter les caractéristiques des objets que l'on cherche à classifier dans un espace au sein duquel leur classification est possible à l'aide de techniques de calcul efficaces.

\subsection{Distance et similarité}

En mathématiques, l'idée de distance renvoie à une application bien particulière visant à quantifier la \textit{similarité} entre deux points $x_1$ et $x_2$ du plan ou de l'espace. Une mesure de similarité est appelée \textit{distance} si et seulement si elle respecte les propriétés suivantes :

\begin{itemize}
	\item $d(x_1, x_2) \geq 0, d(x_1, x_2) = 0 \iff x_1 = x_2$ (positivité)
	\item $d(x_1, x_2) = d(x_2, x_1)$ (symmétrie)
	\item $d(x_1, x_3) \leq d(x_1, x_2) + d(x_2, x_3) \forall (x_1, x_2, x_3)$ (inégalité triangulaire)
\end{itemize}

Il existe plusieurs manières de calculer la distance entre deux objets. On utilise l'une d'elles dans la vie de tous les jours sans même s'en apercevoir dans les espaces à deux ou trois dimensions : la distance Euclidienne, définie par la racine de la somme des différences entre les coordonnées des deux objets élevées au carré\footnote{Pour deux objets $x = (x_1, x_2, \dots, x_n)$ et $y = (y_1, y_2, \dots, y_n)$, la distance qui les sépare vaut $\sqrt{\sum_{i=1}^n (x_i-y_i)^2}$}.

Ici, la distance utilisée tient compte du fait que les objets à classer ne sont plus de simples points dans l'espace, mais des URL, qui sont en somme des chaînes de caractères. Trois mesures de distance entre deux URL $s_1$ et $s_2$ ont été étudiées : la distance de Levenshtein\footnote{$d_{\text{LVS}} = \begin{cases}
  \max(i,j) & \text{ if } \min(i,j)=0, \\
  \min \begin{cases}
          d_{\text{LVS}}(i-1,j) + 1 \\
          d_{\text{LVS}}(i,j-1) + 1 \\
          d_{\text{LVS}}(i-1,j-1) + 1_{(a_i \neq b_j)}
       \end{cases} & \text{ otherwise.}
\end{cases}$}, la distance de Jaro\footnote{$d_{\text{Jaro}} = \frac{1}{3}\left(\frac{m}{|s_1|} + \frac{m}{|s_2|} + \frac{m-t}{m}\right)$ où $|s_i|$ est la longueur de la chaîne de caractères $s_i$, $m$ est le nombre de ''caractères correspondants'' et $t$ est le nombre de ''transpositions''}, et une version modifiée de la distance de Levenshtein\footnote{$d_{\text{URL} (s_1, s_2)} = 1 - \frac{|s_1| + |s_2| - d_{\text{LVS2}}(s_1, s_2)}{|s_1| + |s_2|}$ où $d_{\text{LVS2}}$ est une modification de $d_{\text{LVS}}$ pondérant les remplacements de caractères par 2 plutôt que par 1}.

Pour ce travail, la dernière mesure de distance $d_{\text{URL}}$ a été retenue.

\subsection{DBSCAN et segmentation}

Une fois la distance entre chaque objet mesurée, il faut constituer automatiquement des groupes au sein desquels celle-ci n'excède pas une valeur seuil choisie avec parcimonie. Si cette valeur seuil est trop élevée, les groupes risquent de se révéler trop grands, englobant ainsi trop d'hétérogénéité. Si elle est au contraire trop petite, les groupes risquent de ne pas capturer assez d'information, et de trop nombreux groupes risquent d'être créés. 

Tout ce processus de construction de groupes porte en apprentissage statistique le nom de \textit{clustering}, et peut être réalisé au moyen de méthodes diverses et variées. Ici, c'est la méthode DBSCAN\footnote{\textit{Density-Based Spatial Clustering of Applications with Noise} est un algorithme utilisant la distance entre deux points et le nombre minimum de points choisis pour constituer un \textit{cluster} comme paramètres de segmentation automatique} qui a été choisie, par souci d'efficacité et de rapidité d'exécution.

Rappelons que l'idée derrière le \textit{clustering} est de créer des groupes d'objets homogènes suffisamment denses pour contenir assez d'information, et suffisamment distincts pour que cette information soit correctement distribuée. DBSCAN fonctionne sur le principe suivant : $r$ est le rayon dans lequel doivent se trouver au moins $MinPts$ points ou plus pour que soit constituée un \textit{cluster}. Pour un nouveau point donné, l'algorithme vérifie par la suite que son $r$-voisinage contient au moins $MinPts$ points. Ce nouveau point appartient donc alors à un nouveau \textit{cluster}, et l'$r$-voisinage est parcouru de proche en proche afin de définir les limites de ce \textit{cluster}.

\subsection{Résultats de \textit{clustering}}

Une étape essentielle dans ce genre de travaux consiste à observer l'influence de la variation des paramètres des algorithmes que l'on utilise sur les résultats obtenus, c'est la \textit{sensitivity analysis}. Ici, les deux paramètres que l'on peut faire varier touchent directement à la taille des \textit{clusters} créés par DBSCAN, il s'agit du rayon $r$ et du nombre de points minimum $MinPoints$ les constituant.

Dans les faits, nous avons obtenu des résultats significatifs et exploitables. Même si l'analyse aurait pu être poussée plus loin, nous nous satisfaisons pour l'instant des résultats que nous avons. On observe une claire variation du nombre de \textit{clusters} créés et de la proportion de points considérés comme du bruit\footnote{Donc pas intégrés à un \textit{cluster}} en fonction du rayon $r$ de \textit{clustering} utilisé (fig.~\ref{clustering}).

\begin{figure}[!h]
	\center
	\includegraphics[scale=0.21]{clustering.png}
	\caption{Résultats de \textit{clustering} par DBSCAN selon différentes valeurs de $r$}
	\label{clustering}
\end{figure}

Précisons d'emblée que chacune des quatre paires de graphes correspond à un \textit{clustering} réalisé sur un nombre croissant d'URL \texttt{Unique URL} en faisant varier le rayon $r$, $MinPoints$ étant maintenu constant égal à 4. Le graphe du haut rend compte de l'évolution du nombre de \textit{clusters} \texttt{N\_clusters} obtenu en fonction de $r$, le graphe du bas rend compte de la proportion d'URL considérées comme du bruit \textit{frac\_noise} en fonction de $r$. L'idéal serait d'obtenir assez de \textit{clusters} pour séparer efficacement les URL en segments sans toutefois maintenir trop d'URL à l'écart en les considérant comme du bruit.

Ces résultats nous apprennent plusieurs choses. Tout d'abord, la combinaison du calcul de distance entre URL par la fonction de Levenshtein modifiée $d_{\text{LVS2}}$ et du \textit{clustering} par DBSCAN pourrait être adaptée au traitement de notre problématique. Reste à vérifier que les \textit{clusters} créés sur ces données d'entraînement rendent compte d'une certaine homogénéité et restent exploitables dans une optique de ciblage d'utilisateurs (qui reste l'objectif principal, rappelons-le à toutes fins utiles !). Nous y reviendrons.

Ensuite, il semble exister une valeur de $r$ pour laquelle le nombre de \textit{clusters} obtenus est maximal. S'il s'avère que les URL sont correctement distribuées (i.e. si le contenu des \textit{clusters}) nous satisfait, alors on pourrait considérer cette valeur de $r$ comme optimale, dans la mesure où elle permet une segmentation efficace. Là encore, reste à vérifier le contenu des \textit{clusters}.

Enfin, à l'instar de l'évolution du nombre de \textit{clusters}, il semble exister une valeur de $r$ pour laquelle la proportion d'URL considérées comme du bruit est acceptable, par exemple autour de $20$~\%. Reste à placer \texttt{N\_clusters} en regard de \texttt{frac\_noise} et à vérifier que ce \textit{clustering} est satisfaisant dans une optique de ciblage...

\subsection{Limites de la méthode}

Un écueil n'a cependant pu être évité lors de la réalisation de ces différents essais de \textit{clustering}. Le lecteur averti aura en effet remarqué que le temps d'exécution est fourni pour chaque calcul, et qu'il augmente de manière non linéaire avec le nombre d'URL traitées. Ainsi, si $521$ URL peuvent être passées à la moulinette en un peu plus de $15$ minutes, il faut près d'$1$~h pour en traiter le double ($521$ contre $1041$), et il faut presque $9$~h~$30$ pour réaliser un essai de \textit{clustering} sur $3123$ URL (!) 

La conclusion de ces essais est sans appel : dans un optique de ciblage publicitaire, des centaines de milliers d'URL devront être traitées chaque jour, un temps de calcul aussi long que celui observé ici pour calibrer notre modèle n'est pas envisageable. Il faut donc s'orienter vers une méthode plus adaptée au traitement de grands volumes de données.

\subsection{Optimisations possibles}

D'une façon générale, une utilisation abusive de la bibliothèque \texttt{Pandas} entraîne une diminution des performances des algorithmes lorsqu'on cherche à effectuer ce genre de tâches. Ce point de départ constitue une première orientation pour réécrire un peu le code, mais ce n'est pas tout.

Plusieurs solutions s'offrent à nous, comme se tourner vers le module \texttt{distance} de la bibliothèque \texttt{scipy} (codée en \texttt{C}) plutôt que de coder le calcul des distances en \texttt{Python} brut ; utiliser préférentiellement \texttt{numpy}, plus adapté aux calculs matriciels dont nous avons fait grand usage ici ; et précompiler les fonctions de calculs de distances en \texttt{Jit}. Un premier exemple des différences de temps de calcul entre une fonction codée en \texttt{Python} et une fonction précompilée en \texttt{Jit} est obtenu sur $10$ URL, le résultat est stupéfiant (fig.~\ref{times}).

\begin{figure}[!h]
	\center
	\includegraphics[scale=0.5]{times.png}
	\caption{Comparaison des temps de calcul entre fonction brute (en bas) et fonction précompilée (en haut)}
	\label{times}
\end{figure}

Pour aller plus loin, nous avons essayé la bibliothèque \texttt{Polyleven}\footnote{http://ceptord.net/20181215-polyleven.html}. Les gains en termes de temps d'exécution ne laissent aucune place à la comparaison... (fig.~\ref{polyleven}).

\begin{figure}[!h]
	\center
	\includegraphics[scale=2.1]{polyleven.png}
	\caption{Résultats de \textit{clustering} par DBSCAN selon différentes valeurs de $r$ après utilisation de la bibliothèque \texttt{Polyleven}}
	\label{polyleven}
\end{figure}

\newpage
\section{Approche sémantique de l'analyse des URL et segmentation automatique par la méthode des \textit{n}-grammes}

La dernière approche a fait appel à un concept développé pour le traitement de problématiques liées à la théorie de l'information. Ici, l'idée est de classer automatiquement les URL (de créer des segments) sans avoir à les décortiquer ou à les diviser en leurs caractères constitutifs. Le modèle $n$-grammes s'est révélé plutôt bien adapté.

\subsection{Le modèle $n$-grammes}

D'une manière générale, un $n$-gramme est une séquence de $n$ éléments construite à partir d'une séquence donnée~\cite{abdallah}. On se donne un ensemble de documents $D = \{d_1, d_2, \dots, d_n\}$ et un ensemble de classes auxquelles on cherche à assigner chaque document $C = \{c_1, c_2, \dots, c_k\}$. Pour un document $d_i$, la probabilité $p(c_j|d_i)$ qu'il appartienne à la classe $c_j$ peut s'exprimer à l'aide du théorème de Bayes par :

\[
p(c_j|d_i) = \frac{p(d_i|c_j) \cdot p(c_j)}{p(d_i)}
\]

avec $p(d_i)$ constant pour chaque classe $c_j$, et $p(c_j)$ pouvant être considéré comme la fréquence de la classe $j$ dans les données d'entraînement, par exemple. Reste à traiter le terme $p(d_i|c_j)$... La réalité n'est en fait pas si complexe. Pour une URL constituée par les caractères $w_1, w_2, \dots, w_L$, $p(d_i|c_j)$ s'exprime comme : $p(w_1, w_2, \dots, w_L|c_j)$, et il s'agit en fait de la \textbf{probabilité de voir apparaître la séquence $w_1, w_2, \dots, w_L$ dans la classe $c_j$}, qui peut être calculée selon :

\[
p(w_1, w_2, \dots, w_L|c_j) = \Pi_{i=1}^{L}p((w_i|w_{i-1}, w_{i-2}, \dots, w_1)|c_j)
\]

En fait, on considère la plupart du temps que chaque caractère $w_i$ est uniquement dépendant des $n-1$ caractères précédents, ce qui donne une équation légèrement simplifiée\footnote{Ici, le lecteur pourra remarquer que si cette approximation peut être plausible lorsqu'on travaille avec des documents textes où les $w_i$ représentent des mots, il peut en aller tout autrement lorsque l'on travaille avec des URL, où l'on observe parfois aucune structure syntaxique} :

\[
p(w_1, w_2, \dots, w_L|c_j) = \Pi_{i=1}^{L}p((w_i|w_{i-1}, w_{i-2}, \dots, w_{i-n+1})|c_j)
\]

Le modèle $n$-gramme est donc la distribution de probabilité de chaque séquence de caractères de longueur $n$ basée sur l'ensemble des données d'entraînement disponibles. On dit alors que $p(w_1, w_2, \dots, w_L|c_j)$ est le \textit{le modèle $n$-gramme pour la classe $c_j$}.

\subsection{Construction du modèle $n$-grammes à partir des données d'URL disponibles}

Le principe de construction du modèle repose sur l'utilisation de segments déjà créés par l'équipe Data Analyse. Ces segments d'URL ont été créés manuellement à l'aide de règles syntaxiques simples comme \textit{l'URL comporte les termes "voiture" et "luxe"}. D'une certaine façon, nous disposons déjà de segments d'URL qui peuvent servir de données d'entraînement. 

\subsubsection{Entraînement}
L'objectif est de construire un modèle $n$-gramme pour chaque segment d'URL, qui correspondent en réalité aux classes au sein desquelles il faudra classer toute nouvelle URL automatiquement par la suite. Pour chaque classe dans les données d'entraînement, les probabilités décrites plus haut sont calculées en comptant le nombre d'occurrences de chaque séquence de longueur $n$ et de longueur $n-1$ parmi les URL constituant la classe. Par exemple, imaginons qu'une des classes (rappelons qu'il s'agit des segments manuellement créés par l'équipe Data Analyse dont nous disposons) soit constituée par $c_j = \{\text{'ABCDE'}, \text{'ABC'}, \text{'CDE'}\}$ ; pour construire un modèle $3$-grammes sur cette classe, on s'intéresse aux séquences de caractères de longueur $3$ et de longueur $2$ et à leurs occurrences : 

3-grammes : ($\text{'ABC'}$: 2 $\text{occurrences}$), ($\text{'BCD'}$: 1 $\text{occurrences}$), ($\text{'CDE'}$: 2 $\text{occurrences}$)

2-grammes : ($\text{'AB'}$: 2 $\text{occurrences}$), ($\text{'BC'}$: 1 $\text{occurrences}$), ($\text{'CD'}$: 2 $\text{occurrences}$), ($\text{'DE'}$: 2 $\text{occurrences}$)

\subsubsection{Test}

Le décompte d'occurrences pour chaque séquence constituait la phase d'entraînement des modèles. La phase de test consiste à :

\begin{itemize}
	\item convertir chaque URL en $n$-grammes
	\item pour chaque $n$-gramme, calculer sa probabilité d'occurrence dans cette classe
	
	$p((w_i|w_{i-n+1}^{i-1})|c_j) = \frac{count(w_{i-n+1}^{i})}{count(w_{i-n+1}^{i-1})}$
\end{itemize}

Chaque nouvelle URL $URL_i$ est ensuite classée comme partie de la classe $c_j$ si et seulement si le modèle $n$-grammes de cette classe renvoie la valeur maximale de $p(c_j|URL_i)$.

\subsection{Segmentation d'URL}

\subsubsection{Analyse exploratoire - Distribution des URL}

L'analyse des données à disposition montre que l'on possède (pour les tests réalisés en juillet 2020) $18623$ URL uniques réparties de façon très hétérogène en $25$ classes (fig.~\ref{distribution}).

\begin{figure}[!h]
	\center
	\includegraphics[scale=0.29]{distribution}
	\caption{Distribution des URL des données au sein des différentes classes. Les classes sont repérées par leur identifiant, relié ensuite à leur nom grâce à une table de correspondance}
	\label{distribution}
\end{figure}

\subsubsection{Construction des jeux d'entraînement et de test}

Les données disponibles sont séparées en un jeu d'entraînement, qui sert à construire les modèles $n$-grammes, et un jeu de test, conservé à l'écart dans un premier temps. Ce jeu de test servira, une fois les modèles construits, à vérifier leur pertinence et la reproductibilité des résultats.

Le jeu d'entraînement est constitué par 66~\% des données disponibles, échantillonnées aléatoirement. Le jeu de test est constitué par les 33~\% des données restantes\footnote{Cette distribution 2/3-1/3 entre entraînement est test résulte d'une convention généralement suivie lorsqu'on segmente un jeu de données pour construire un modèle et le tester. Pour aller plus loin, on peut ajouter une partie servant à la validation, selon une partition 60~\%, 20~\%, 20~\% entre entraînement, validation et test, mais nous n'en avons pas fait usage ici}. 

Les données d'entraînement comportent donc $12477$ URL distribuées en $25$ classes (sans surprise). Les données de test comportent $6146$ distribuées en $25$ classes (figs.~\ref{distribution_train}~et~\ref{distribution_test}).

\begin{figure}[!h]
	\center
	\includegraphics[scale=0.29]{distribution_train.png}
	\caption{Distribution des URL des données d'entraînement au sein des différentes classes}
	\label{distribution_train}
\end{figure}

\begin{figure}[!h]
	\center
	\includegraphics[scale=0.29]{distribution_test.png}
	\caption{Distribution des URL des données de test au sein des différentes classes}
	\label{distribution_test}
\end{figure}

Il est essentiel d'observer que l'on conserve bien l'ensemble des classes des données initiales dans les deux jeux de données construits ; et là encore, de nombreuses améliorations sont possibles, comme par exemple s'assurer au moyen de tests statistiques que la distribution des URL dans les classes est identique au sein des données d'entraînement et de test. Ici, nous nous sommes contentés d'une observation des diagrammes de distribution dans chaque jeu de données...

\subsubsection{Implémentation du modèle $n$-grammes}

L'implémentation du modèle $n$-grammes pour la segmentation automatique d'URL a fait appel à certains modules de la bibliothèque Python (dont le code a été légèrement revu) \texttt{irlib}\footnote{https://github.com/gr33ndata/irlib} développée par Tarek Amr Abdallah.

Comme pour de nombreux projets informatiques d'apprentissage statistique, les étapes à suivre une fois les données d'entraînement et de test construites consistent principalement en la définition d'un modèle dont les paramètres sont renseignés par l'utilisateur et appelés à être affinés selon les résultats obtenus en phase de test (ou de validation).

\subsection{Premiers résultats de segmentation}

\newpage
\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{bibliographie}

\end{document}